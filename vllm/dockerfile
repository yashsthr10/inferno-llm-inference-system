FROM nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive

# Install Python manually since CUDA base image doesn't have it
RUN apt update && apt install -y python3 python3-pip git curl

# Optional: Symlink python â†’ python3
RUN ln -s /usr/bin/python3 /usr/bin/python

# Install packages
RUN pip install --upgrade pip
# <<< CHANGE: Added auto-gptq for broader quantization support
RUN pip install vllm[transformers] torch torchvision huggingface_hub \
    grpcio grpcio-tools httpx auto-gptq

# Huggingface cache for model weights
RUN mkdir -p /root/.cache/huggingface

# <<< CHANGE: Switched to a pre-quantized AWQ model from Hugging Face
ENV MODEL_NAME="TheBloke/Mistral-7B-Instruct-v0.2-AWQ"
ENV GPU_MEMORY_UTILIZATION="0.9"

EXPOSE 8000
WORKDIR /app

COPY . .

# <<< CHANGE: Switched to awq and adjusted max_model_len. Added --dtype to prevent errors.
# Entrypoint for vLLM and gRPC
CMD ["sh", "-c", "huggingface-cli login --token $HUGGINGFACE_TOKEN && python3 -m vllm.entrypoints.openai.api_server --model $MODEL_NAME --served-model-name my-quantized-model --gpu-memory-utilization $GPU_MEMORY_UTILIZATION --quantization awq --max-model-len 4096 --dtype float16"]
